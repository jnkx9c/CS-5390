{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 26078
    },
    "colab_type": "code",
    "id": "ar1_a9YGGt-1",
    "outputId": "b95aed45-d1db-4079-eb0a-25357b4cd179",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating datafrom from file.\n",
      "dataframe was created from file\n",
      "2018-12-31 20:30:00\n",
      "adding shifts\n",
      "searching for start/end dates\n",
      "startDate 2015-01-01\n",
      "endDate 2018-12-31\n",
      "starting to fill in the gaps\n",
      "      Year  Month  Day  Neighborhood  Shift  count\n",
      "0     2015      1    1             1      0      1\n",
      "1     2015      1    1             1      1      3\n",
      "2     2015      1    1             1      2      2\n",
      "3     2015      1    2             1      0      2\n",
      "4     2015      1    2             1      1      2\n",
      "5     2015      1    2             1      2      2\n",
      "6     2015      1    3             1      1      6\n",
      "7     2015      1    4             1      0      1\n",
      "8     2015      1    4             1      2      1\n",
      "9     2015      1    5             1      0      1\n",
      "10    2015      1    5             1      1      4\n",
      "11    2015      1    5             1      2      1\n",
      "12    2015      1    6             1      0      3\n",
      "13    2015      1    6             1      1      1\n",
      "14    2015      1    7             1      0      1\n",
      "15    2015      1    7             1      2      1\n",
      "16    2015      1    8             1      1      1\n",
      "17    2015      1    8             1      2      3\n",
      "18    2015      1    9             1      2      3\n",
      "19    2015      1   10             1      0      2\n",
      "20    2015      1   11             1      1      4\n",
      "21    2015      1   11             1      2      2\n",
      "22    2015      1   12             1      0      1\n",
      "23    2015      1   12             1      1      1\n",
      "24    2015      1   12             1      2      1\n",
      "25    2015      1   13             1      1      4\n",
      "26    2015      1   13             1      2      1\n",
      "27    2015      1   14             1      2      3\n",
      "28    2015      1   15             1      0      2\n",
      "29    2015      1   15             1      1      4\n",
      "...    ...    ...  ...           ...    ...    ...\n",
      "8736  2018     12   23             1      1      0\n",
      "8737  2018     12   24             2      0      0\n",
      "8738  2018     12   24             2      1      0\n",
      "8739  2018     12   24             2      2      0\n",
      "8740  2018     12   24             1      0      0\n",
      "8741  2018     12   24             1      2      0\n",
      "8742  2018     12   25             2      0      0\n",
      "8743  2018     12   25             2      1      0\n",
      "8744  2018     12   25             2      2      0\n",
      "8745  2018     12   25             1      0      0\n",
      "8746  2018     12   26             2      1      0\n",
      "8747  2018     12   26             2      2      0\n",
      "8748  2018     12   27             2      1      0\n",
      "8749  2018     12   27             2      2      0\n",
      "8750  2018     12   27             1      0      0\n",
      "8751  2018     12   27             1      2      0\n",
      "8752  2018     12   28             2      0      0\n",
      "8753  2018     12   28             2      1      0\n",
      "8754  2018     12   28             2      2      0\n",
      "8755  2018     12   29             2      0      0\n",
      "8756  2018     12   29             2      1      0\n",
      "8757  2018     12   29             2      2      0\n",
      "8758  2018     12   29             1      0      0\n",
      "8759  2018     12   29             1      1      0\n",
      "8760  2018     12   30             2      0      0\n",
      "8761  2018     12   30             2      1      0\n",
      "8762  2018     12   30             2      2      0\n",
      "8763  2018     12   31             2      0      0\n",
      "8764  2018     12   31             2      1      0\n",
      "8765  2018     12   31             1      1      0\n",
      "\n",
      "[8766 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import datetime\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "\n",
    "def determineShift(hour):\n",
    "    if 0<=hour<8:\n",
    "        return 0\n",
    "    if 8<=hour<16:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "crime_file = '/tmp/STL_Crime_2015-2018.csv'\n",
    "usecols=['DateOccur', 'Neighborhood']\n",
    "\n",
    "print(\"creating datafrom from file.\")\n",
    "df = pd.read_csv(crime_file,usecols=usecols,parse_dates= ['DateOccur',])\n",
    "print(\"dataframe was created from file\")\n",
    "#remove all neighorhoods but one for debugging\n",
    "df.drop(df[df.Neighborhood > 2].index, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "#Remove any rows that have effed up coordinates\n",
    "df.drop(df[df.Neighborhood == 0].index,inplace=True)\n",
    "\n",
    "print (df.DateOccur[0])\n",
    "\n",
    "#convert the string-typed 'DateOccur' to a datetime\n",
    "#print(\"converting DateOccur to datetime\")\n",
    "#df['DateOccur'] = pd.to_datetime(df['DateOccur'])\n",
    "print(\"adding shifts\")\n",
    "df['Shift'] = df['DateOccur'].map(lambda x: determineShift(x.hour))\n",
    "print(\"searching for start/end dates\")\n",
    "start_date = df['DateOccur'].min().date()\n",
    "end_date = df['DateOccur'].max().date()\n",
    "print(\"startDate\",start_date);\n",
    "print(\"endDate\",end_date);\n",
    "\n",
    "\n",
    "df['just_date'] = df['DateOccur'].dt.date\n",
    "g1 = pd.DataFrame({'count' : df.groupby( [ \"Neighborhood\",\"just_date\",\"Shift\"] ).size()}).reset_index()\n",
    "\n",
    " \n",
    "#need to fill in the blanks, for every date/shift/neighborhood that is missing from the dataset, add a count of 0\n",
    "#set the index of the dataframe to neighborhood,date, shift (multiindex)\n",
    "g1.set_index(['just_date','Neighborhood','Shift'], inplace=True)\n",
    "d = start_date\n",
    "delta = datetime.timedelta(days=1)\n",
    "nbhds = df['Neighborhood'].unique();\n",
    "gap_rows_list = []\n",
    "print(\"starting to fill in the gaps\")\n",
    "while d <= end_date:\n",
    "  #print(d)\n",
    "  for n in nbhds:\n",
    "    #print(' '+str(n))\n",
    "    for s in range(0,3):\n",
    "      #print('  s')\n",
    "      #check if the index is in the dataframe\n",
    "      if g1.index.isin([(d, n, s)]).any() == False:\n",
    "        #the index wasn't found in the frame.  Add a row to the gap_list with a count of 0 \n",
    "        gap_rows_list.append({'Neighborhood':n,'just_date':d,'Shift':s,'count':0})\n",
    "  d += delta\n",
    "#at this point, gap_rows_list should contain a row for each day/neighborhood/shift with a count=0\n",
    "#Create a dataframe from gap_rows_list\n",
    "gap_df = pd.DataFrame( gap_rows_list)\n",
    "g1.reset_index(inplace=True)\n",
    "full_df=g1.append(gap_df,ignore_index=True)\n",
    "#full_df should now have a row for each date,neigborhood,shift with a crime count\n",
    "#this is the df that we should be working with going forward.\n",
    "#delete any other dfs we have created.\n",
    "\n",
    "del df\n",
    "del g1\n",
    "del gap_df\n",
    "\n",
    "\n",
    "#break up the just_date into Month,Day,Year columns\n",
    "full_df['Month'] = full_df['just_date'].map(lambda x: x.month)\n",
    "full_df['Year'] = full_df['just_date'].map(lambda x: x.year)\n",
    "full_df['Day'] = full_df['just_date'].map(lambda x: x.day)\n",
    "full_df.drop(columns=['just_date'],inplace=True)\n",
    "\n",
    "#rearrange the columns to make some order\n",
    "full_df = full_df[['Year','Month','Day','Neighborhood','Shift','count']]\n",
    "\n",
    "print(full_df)\n",
    "full_df.to_csv('out.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nnRxVUDK5Myv",
    "outputId": "a32c5cb7-8b2e-4991-fc7a-8eb1fb3cfb9d",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4762ae55d864>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"out.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "dataset = np.loadtxt(\"out.csv\", delimiter=\",\",skiprows=1,dtype=int )\n",
    "\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "print (len(dataset))\n",
    "trainSize = int(round(len(dataset)*.3))\n",
    "valSize = int(round(len(dataset)*.2))\n",
    "testSize = int(round(len(dataset)*.5))\n",
    "\n",
    "\n",
    "XTRAIN = dataset[:trainSize,0:5]\n",
    "YTRAIN = dataset[:trainSize,5]\n",
    "XVALIDATION = dataset[trainSize:valSize,0:5]\n",
    "YVALIDATION = dataset[trainSize:valSize,5]\n",
    "XTEST = dataset[:testSize,0:5]\n",
    "YTEST = dataset[:testSize,5]\n",
    "\n",
    "print(XTRAIN)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='sigmoid'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "print(model.summary())\n",
    "\n",
    "#print(XTRAIN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARgH8TtP5Uz5"
   },
   "source": [
    "Split up data into training data, validation data, and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXCfVLbI5SGp"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qdQmVCt05Spx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nplNaoFpAj0L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STL-Crime",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
